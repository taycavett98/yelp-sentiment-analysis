{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install pandas\n",
    "!{sys.executable} -m pip install seaborn\n",
    "!{sys.executable} -m pip install nltk\n",
    "!{sys.executable} -m pip install scikit-learn\n",
    "!{sys.executable} -m pip install imblearn\n",
    "!{sys.executable} -m pip install imbalanced-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# read in data\n",
    "train_data = pd.read_csv('train.csv')\n",
    "test_data = pd.read_csv('test.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize Data Before Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Unnamed: 0  label                                               text\n",
      "0           0      4  dr. goldberg offers everything i look for in a...\n",
      "1           1      1  Unfortunately, the frustration of being Dr. Go...\n",
      "2           2      3  Been going to Dr. Goldberg for over 10 years. ...\n",
      "3           3      3  Got a letter in the mail last week that said D...\n",
      "4           4      0  I don't know what Dr. Goldberg was like before...\n",
      "(650000, 3)\n",
      "Training examples: 650000, Testing examples: 50000, Attributes: 1\n",
      "          Unnamed: 0          label\n",
      "count  650000.000000  650000.000000\n",
      "mean   324999.500000       2.000000\n",
      "std    187638.981824       1.414215\n",
      "min         0.000000       0.000000\n",
      "25%    162499.750000       1.000000\n",
      "50%    324999.500000       2.000000\n",
      "75%    487499.250000       3.000000\n",
      "max    649999.000000       4.000000\n",
      "label\n",
      "4    130000\n",
      "1    130000\n",
      "3    130000\n",
      "0    130000\n",
      "2    130000\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# explore data\n",
    "print(train_data.head())\n",
    "print(train_data.shape)\n",
    "\n",
    "# subtract 2 from the number of columns to account for the 'id' and 'label'(target) columns\n",
    "print(f\"Training examples: {train_data.shape[0]}, Testing examples: {test_data.shape[0]}, Attributes: {train_data.shape[1] - 2}\")\n",
    "print(train_data.describe())\n",
    "print(train_data['label'].value_counts())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Labeled Review Chart\n",
    " The chart below shows uniform distribution of sentiment analysis and class balance, showing that the model will not be more biased towared more common classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Quick EDA\n",
    "ax = train_data['label'].value_counts().sort_index().plot(kind='bar', title=\"Count of Labeled Reviews\", figsize=(10, 5))\n",
    "ax.set_xlabel(\"Labeled Reviews\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/tay/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "def preprocess_text_1(text):\n",
    "    '''\n",
    "    this function removes punctuation, converts text to lowercase, removes stopwords, and returns the cleaned text\n",
    "    in the form of a string\n",
    "    '''\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text, re.I|re.A)\n",
    "    text = text.lower()\n",
    "    text = text.split()\n",
    "    text = [word for word in text if word not in stop_words]\n",
    "    text = ' '.join(text)\n",
    "    return text\n",
    "\n",
    "train_data['text'] = train_data['text'].apply(preprocess_text_1)\n",
    "test_data['text'] = test_data['text'].apply(preprocess_text_1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "next, we will convert text to numerical representations\n",
    "\n",
    "why?\n",
    "- most ML algorithms will be working with numerical data as input so the machine can understand the data and use it for the algorithm\n",
    "- this conversion is called feature extraction or vectorization\n",
    "\n",
    "why are we using TF-IDF below?\n",
    "- Term Frequency-Inverse Document Frequency is a popular method for converting text into numerical vectors. it assigns weights to each word based on how important or relevant it is in the corpus. words that appear frequently in a specific document (text review) but infrequently across the entire corpus are given higher weights, as they are more informative for that specific document.\n",
    "- TF-IDF is widely used because it captures the relative importance of words in a doc., which can be beneficial for tasks like sentiment analysis, where the presence of certain words can strongly indicate the sentiment.\n",
    "\n",
    "what are other models we could use, and would it be smart to test out using different methods?\n",
    "- i want to compare these different text vectorization methods\n",
    "    1. bag-of-words : simple method, counts the occurrences of each word in a document, representing it as a vector of word frequencies\n",
    "    2. word embeddings : method represents words as dense vectors that capture semantic and syntactic info, such as Word2Vec or GloVe embeddings.\n",
    "    3. n-gram models : instead of considering single words, n-grams capture sequences of n consecutive words, which can provide more context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "'''this converts text into TF-IDF vectors, which are used as features for the model'''\n",
    "vectorizer = TfidfVectorizer()\n",
    "X_train = vectorizer.fit_transform(train_data['text'])\n",
    "X_test = vectorizer.transform(test_data['text'])\n",
    "y_train = train_data['label']\n",
    "y_test = test_data['label']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validation Set\n",
    "we want to separate the data into a validation set to tune hyperparameters and evaluate model's performance during training. this validation set acts as an unbiased proxy for the test set, allowing you to tune the model's hyperparmaters (regularization strength, number of trees, etc.) and prevent overfitting to the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets separate the training data into a validation set\n",
    "from sklearn.model_selection import train_test_split\n",
    "# we use random_state to ensure reproducibility - it controls the randomness of the split. 42 is a common value\n",
    "# you guarantee that the output of Run 1 will be equal to the output of Run 2, i.e. your split will be always the same\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizing the Preprocessed Data\n",
    "to get a better understanding of what it looks like before moving on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first 5 examples from training data\n",
      "                                                text  label\n",
      "0  dr goldberg offers everything look general pra...      4\n",
      "1  unfortunately frustration dr goldbergs patient...      1\n",
      "2  going dr goldberg years think one st patients ...      3\n",
      "3  got letter mail last week said dr goldberg mov...      3\n",
      "4  dont know dr goldberg like moving arizona let ...      0\n",
      "\n",
      "shape of training data: (520000, 790337)\n",
      "shape of validation data: (130000, 790337)\n",
      "shape of test data: (50000, 790337)\n",
      "\n",
      "lbel distribution in training data\n",
      "label\n",
      "4    130000\n",
      "1    130000\n",
      "3    130000\n",
      "0    130000\n",
      "2    130000\n",
      "Name: count, dtype: int64\n",
      "label distribution in test data\n",
      "label\n",
      "0    10000\n",
      "2    10000\n",
      "1    10000\n",
      "3    10000\n",
      "4    10000\n",
      "Name: count, dtype: int64\n",
      "\n",
      "example vectors from training data:\n",
      "  (0, 14481)\t0.3521445436384422\n",
      "  (0, 520172)\t0.22519277351182185\n",
      "  (0, 506118)\t0.21425344076291708\n",
      "  (0, 438206)\t0.17074109239568955\n",
      "  (0, 747013)\t0.1685015538587805\n",
      "  (0, 358168)\t0.3609007648115094\n",
      "  (0, 54517)\t0.15996351199765999\n",
      "  (0, 499107)\t0.24931621932952291\n",
      "  (0, 564629)\t0.20606683799292955\n",
      "  (0, 232179)\t0.13532308249296385\n",
      "  (0, 134554)\t0.1274621735456892\n",
      "  (0, 83958)\t0.13669152720899022\n",
      "  (0, 566327)\t0.14883653891337964\n",
      "  (0, 523862)\t0.12079913101974117\n",
      "  (0, 255623)\t0.15039573303910733\n",
      "  (0, 427848)\t0.15942199875505386\n",
      "  (0, 58671)\t0.12997296975718328\n",
      "  (0, 785180)\t0.16777062855505187\n",
      "  (0, 50920)\t0.31778265816435697\n",
      "  (0, 386689)\t0.09970321479930827\n",
      "  (0, 722438)\t0.0998082799611416\n",
      "  (0, 358477)\t0.14440773173282168\n",
      "  (0, 521051)\t0.18638533245425673\n",
      "  (0, 978)\t0.13730799308412822\n",
      "  (0, 393190)\t0.0982647927295621\n",
      "  :\t:\n",
      "  (1, 317442)\t0.36643984415494835\n",
      "  (1, 735458)\t0.3593599260456706\n",
      "  (1, 761406)\t0.4980767539722179\n",
      "  (1, 250076)\t0.14072603282361032\n",
      "  (1, 611112)\t0.1598928104637537\n",
      "  (1, 736419)\t0.2032825854184853\n",
      "  (1, 598354)\t0.22445479778181895\n",
      "  (1, 100525)\t0.24385353276577584\n",
      "  (1, 428924)\t0.20223050543430227\n",
      "  (1, 285538)\t0.16252405073164186\n",
      "  (2, 248075)\t0.48218829394591156\n",
      "  (2, 248213)\t0.406957637271199\n",
      "  (2, 514055)\t0.21957807284819386\n",
      "  (2, 774493)\t0.3699682185417808\n",
      "  (2, 585805)\t0.2346171349005564\n",
      "  (2, 776103)\t0.2283255036926025\n",
      "  (2, 379927)\t0.15585641902122133\n",
      "  (2, 228920)\t0.2354610532154679\n",
      "  (2, 522364)\t0.1036822035046063\n",
      "  (2, 45056)\t0.12577990572058886\n",
      "  (2, 220448)\t0.17611674005990063\n",
      "  (2, 487184)\t0.23519200442031307\n",
      "  (2, 277797)\t0.1618869280575599\n",
      "  (2, 516540)\t0.24761394297695696\n",
      "  (2, 226350)\t0.17162470676698882\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nprint the first 3 examples. each sample is represented as a vector with columns.\\nX_train is the vectorized text data, each row represents on text sample (review) and each column represents a feature (word in the TF-IDF vector)\\n[:3] is the slicing which selects the first 3 samples\\nthe multiple lines for each sample is that the vectors are high dimension and python prints them on on multiple lines for readability\\n'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('first 5 examples from training data')\n",
    "#print(X_train[:5])\n",
    "print(train_data[['text', 'label']].head())\n",
    "print()\n",
    "print(f'shape of training data: {X_train.shape}')\n",
    "print(f'shape of validation data: {X_val.shape}')\n",
    "print(f'shape of test data: {X_test.shape}')\n",
    "print()\n",
    "print('lbel distribution in training data')\n",
    "print(train_data['label'].value_counts())\n",
    "print('label distribution in test data')\n",
    "print(test_data['label'].value_counts())\n",
    "print()\n",
    "print('example vectors from training data:')\n",
    "print(X_train[:3])\n",
    "'''\n",
    "print the first 3 examples. each sample is represented as a vector with columns.\n",
    "X_train is the vectorized text data, each row represents on text sample (review) and each column represents a feature (word in the TF-IDF vector)\n",
    "[:3] is the slicing which selects the first 3 samples\n",
    "the multiple lines for each sample is that the vectors are high dimension and python prints them on on multiple lines for readability\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree Classifer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "dt_model = DecisionTreeClassifier()\n",
    "dt_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate Model Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3887307692307692\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.53      0.53      0.53     26100\n",
      "           1       0.31      0.31      0.31     25992\n",
      "           2       0.31      0.31      0.31     26130\n",
      "           3       0.32      0.32      0.32     26048\n",
      "           4       0.46      0.47      0.47     25730\n",
      "\n",
      "    accuracy                           0.39    130000\n",
      "   macro avg       0.39      0.39      0.39    130000\n",
      "weighted avg       0.39      0.39      0.39    130000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "predictions = dt_model.predict(X_val)\n",
    "print(accuracy_score(y_val, predictions))\n",
    "print(classification_report(y_val, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results Analysis\n",
    "The performance shows similar results across each class, which means this is balanced and the model is not biased towards any class. This could mean, however, that the model can't really distinguish between the classes. The overall accuracy is about 38.87%.\n",
    "\n",
    "## Model Complexity\n",
    "Reasons for this could be model complexity.  If the tree is too complex, it could be overfitting. Pruning the tree would help here. To prune the tree we could limit the depth or set a minimum samples per leaf. If the tree is too simple, it's not capturing necessary patterns in data. To ensure more patterns are captured, we could increase the depth.\n",
    "\n",
    "## Feature Engineering\n",
    "We could try to use better features that could capture the sentiment more effectively:\n",
    "- N-grams : use bi-grams or tri-grams to capture context\n",
    "- Word Embeddings: use pre-trained embeddings like Word2Vec, GloVe, or BERT embeddings \n",
    "\n",
    "## Hyperparameter Tuning\n",
    "We can try using a systematic search for the best hyperparameters using grid search or random search \n",
    "- Grid Search : test randge of values for parameters: 'max_depth', 'min_samples_split', and 'min_samples_leaf'\n",
    "- Random Search : sample wider range of values. typically this is more efficient than grid search\n",
    "\n",
    "## Other Things To Try:\n",
    "- Remove stop words\n",
    "- use stemming or lemmatization, which reduce words to their root form\n",
    "    - use nltk.stem.WordNetLemmatizer\n",
    "- Instead of removing all characters except letters, include punctuation like '?' or '!' which can express sentiment\n",
    "- Tweak the parameters of the TF-IDF like 'ngram_range', 'max_df', 'min_df', and 'max_features'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation of Decision Tree Take 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /Users/tay/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import re\n",
    "import pandas as pd\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "nltk.download('wordnet')\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# reload original data\n",
    "# TODO: could read in once and every time I apply a preprocessing function, name the variable after which preprocessing function was applied\n",
    "# that way I can keep track of the data at each stage of preprocessing and just load the original data once\n",
    "train_data = pd.read_csv('train.csv')\n",
    "test_data = pd.read_csv('test.csv')\n",
    "\n",
    "def preprocess_text_2(text):\n",
    "    '''\n",
    "    this function removes all non-alphabetic characters and leaves only words. \n",
    "    it then coverts all text to lowercase for consistency and splits the text into words.\n",
    "    it then lemmatizes the words to their root form.\n",
    "    then it joins the words back together into a single string and returns the cleaned text\n",
    "    '''\n",
    "    text = re.sub(r'[\\d\\W]+', ' ', text)\n",
    "    text = text.lower().split()\n",
    "    text = [lemmatizer.lemmatize(word) for word in text]\n",
    "\n",
    "    return ' '.join(text)\n",
    "\n",
    "train_data['text'] = train_data['text'].apply(preprocess_text_2)\n",
    "test_data['text'] = test_data['text'].apply(preprocess_text_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF-IDF Vectorization with bigrams and feature selection\n",
    "# TF-IDF takes text data and converts it into a high dimensional vector representation\n",
    "# set up to use uni-grams and bi-grams, remove words that appear in more than 95% of the documents, \n",
    "#    remove words that appear in less than 5 documents, and limit the number of features to 5000\n",
    "# these 5000 dimensions are the \"top\" features as determined by the TF-IDF vectorizer\n",
    "print(\"Starting TF-IDF Vectorization...\")\n",
    "vectorizer = TfidfVectorizer(ngram_range=(1, 2), max_df=0.95, min_df=5, max_features=5000)\n",
    "X_train = vectorizer.fit_transform(train_data['text'])\n",
    "X_test = vectorizer.transform(test_data['text'])\n",
    "print(\"Completed TF-IDF Vectorization.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dimensionality Reduction\n",
    "# do this reduce the feature space which can help the model generalize better. this is making the model less complex\n",
    "# so we are further reducing the dimnesionality of the data from 5000 to 1000 while trying to preserve as much information as possible\n",
    "from sklearn.decomposition import IncrementalPCA\n",
    "\n",
    "print(\"Starting Dimensionality Reduction with IncrementalPCA...\")\n",
    "n_batches = 1000  # or another number that fits your memory constraints\n",
    "ipca = IncrementalPCA(n_components=1000, batch_size=n_batches)\n",
    "X_train_ipca = ipca.fit_transform(X_train)\n",
    "print(f\"Explained variance ratio: {ipca.explained_variance_ratio_.sum()}\")\n",
    "X_test_ipca = ipca.transform(X_test)\n",
    "print(\"Completed Dimensionality Reduction.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting TF-IDF Vectorization...\n",
      "Completed TF-IDF Vectorization.\n",
      "Starting Dimensionality Reduction with IncrementalPCA...\n",
      "Explained variance ratio: 0.5741865389670022\n",
      "Completed Dimensionality Reduction.\n",
      "Splitting the data into training and validation sets...\n",
      "Completed splitting.\n",
      "Starting Hyperparameter Tuning with GridSearchCV...\n",
      "Completed Hyperparameter Tuning.\n",
      "Evaluating the best model...\n",
      "Accuracy: 0.3540153846153846\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.47      0.49      0.48     26000\n",
      "           1       0.29      0.30      0.29     26000\n",
      "           2       0.29      0.30      0.29     26000\n",
      "           3       0.30      0.29      0.29     26000\n",
      "           4       0.44      0.40      0.42     26000\n",
      "\n",
      "    accuracy                           0.35    130000\n",
      "   macro avg       0.36      0.35      0.35    130000\n",
      "weighted avg       0.36      0.35      0.35    130000\n",
      "\n",
      "Refitting the best model on the entire dataset...\n",
      "Completed refitting.\n"
     ]
    }
   ],
   "source": [
    "# Splitting the data into training and validation sets with stratification\n",
    "print(\"Splitting the data into training and validation sets...\")\n",
    "X_train_ipca, X_val_ipca, y_train, y_val = train_test_split(X_train_ipca, train_data['label'], test_size=0.2, random_state=42, stratify=train_data['label'])\n",
    "print(\"Completed splitting.\")\n",
    "\n",
    "# Hyperparameter tuning\n",
    "print(\"Starting Hyperparameter Tuning with GridSearchCV...\")\n",
    "dt_params = {\n",
    "    'max_depth': 20,\n",
    "    'min_samples_split': 10,\n",
    "    'min_samples_leaf': 4,\n",
    "    'criterion': 'gini'\n",
    "}\n",
    "dt_model = DecisionTreeClassifier(**dt_params, random_state=42)\n",
    "#clf = GridSearchCV(dt, dt_params, cv=3, scoring='accuracy', n_jobs=7)\n",
    "#clf.fit(X_train_ipca, y_train)\n",
    "dt_model.fit(X_train_ipca, y_train)\n",
    "print(\"Completed Hyperparameter Tuning.\")\n",
    "\n",
    "print(\"Evaluating the best model...\")\n",
    "#best_dt_model = clf.best_estimator_\n",
    "#predictions = best_dt_model.predict(X_val_ipca)\n",
    "predictions = dt_model.predict(X_val_ipca)\n",
    "print(f\"Accuracy: {accuracy_score(y_val, predictions)}\")\n",
    "print(classification_report(y_val, predictions))\n",
    "\n",
    "print(\"Refitting the best model on the entire dataset...\")\n",
    "#best_dt_model.fit(X_train_ipca, y_train)\n",
    "print(\"Completed refitting.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.5188769230769231\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.66      0.66     26000\n",
      "           1       0.45      0.45      0.45     26000\n",
      "           2       0.43      0.42      0.43     26000\n",
      "           3       0.43      0.45      0.44     26000\n",
      "           4       0.63      0.61      0.62     26000\n",
      "\n",
      "    accuracy                           0.52    130000\n",
      "   macro avg       0.52      0.52      0.52    130000\n",
      "weighted avg       0.52      0.52      0.52    130000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "mlp = MLPClassifier(hidden_layer_sizes=(512, 256, 128),  # 3 layers with 512, 256, and 128 neurons each\n",
    "                    activation='relu',\n",
    "                    solver='adam',\n",
    "                    alpha=1e-4,  # regularization - L2 penalty (regularization term) parameter which helps in avoiding overfitting by penalizing weights with large magnitudes\n",
    "                    batch_size=256,  # size of minibatches for stochastic optimizers - used in the optimization process to help converge faster\n",
    "                    learning_rate_init=0.001,  # learning curve - step size in updating the weights\n",
    "                    max_iter=200,\n",
    "                    random_state=42)\n",
    "\n",
    "# train model on the training data\n",
    "mlp.fit(X_train_ipca, y_train)\n",
    "\n",
    "# evaluate the model\n",
    "predictions = mlp.predict(X_val_ipca) \n",
    "print(f\"Accuracy: {accuracy_score(y_val, predictions)}\")\n",
    "print(classification_report(y_val, predictions))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.5945538461538461\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      0.76      0.73     26000\n",
      "           1       0.53      0.50      0.52     26000\n",
      "           2       0.51      0.52      0.52     26000\n",
      "           3       0.53      0.46      0.49     26000\n",
      "           4       0.66      0.73      0.69     26000\n",
      "\n",
      "    accuracy                           0.59    130000\n",
      "   macro avg       0.59      0.59      0.59    130000\n",
      "weighted avg       0.59      0.59      0.59    130000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mlp_2 = MLPClassifier(hidden_layer_sizes=(512, 128, 64),  # More layers and different sizes\n",
    "                    activation='tanh',                 # Changing the activation function\n",
    "                    solver='adam',                     # Keeping 'adam' but you can try 'sgd' or 'lbfgs'\n",
    "                    alpha=1e-5,                        # Adjusting regularization\n",
    "                    learning_rate_init=0.001,          # Initial learning rate\n",
    "                    batch_size=128,                    # Smaller batch size\n",
    "                    learning_rate='adaptive',          # Learning rate schedule\n",
    "                    early_stopping=True,               # Enable early stopping\n",
    "                    max_iter=200,                      # Max number of epochs\n",
    "                    random_state=42)\n",
    "\n",
    "# Train and evaluate the MLP as before\n",
    "# Train the MLP on the PCA-reduced data\n",
    "mlp_2.fit(X_train_ipca, y_train)\n",
    "\n",
    "# Evaluate the MLP\n",
    "predictions = mlp_2.predict(X_val_ipca)  # Assuming that you've transformed your validation set with PCA as well\n",
    "print(f\"Accuracy: {accuracy_score(y_val, predictions)}\")\n",
    "print(classification_report(y_val, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the data into training and validation sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, train_data['label'], test_size=0.2, random_state=42, stratify=train_data['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting MLP training...\n",
      "Evaluating the MLP...\n",
      "Accuracy: 0.6117461538461538\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      0.76      0.75     26000\n",
      "           1       0.54      0.58      0.56     26000\n",
      "           2       0.55      0.50      0.52     26000\n",
      "           3       0.55      0.46      0.50     26000\n",
      "           4       0.68      0.75      0.71     26000\n",
      "\n",
      "    accuracy                           0.61    130000\n",
      "   macro avg       0.61      0.61      0.61    130000\n",
      "weighted avg       0.61      0.61      0.61    130000\n",
      "\n",
      "Completed MLP training.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Using raw TF-IDF features - X_train and X_val\n",
    "mlp = MLPClassifier(\n",
    "    hidden_layer_sizes=(1024, 512, 256),  # Expanded capacity with more layers and units\n",
    "    activation='relu',                     # Using ReLU activation function\n",
    "    solver='sgd',                          # Switching to SGD optimizer\n",
    "    alpha=1e-4,                            # Regularization term\n",
    "    batch_size=128,                        # Size of minibatches\n",
    "    learning_rate='adaptive',              # Adaptive learning rate\n",
    "    learning_rate_init=0.01,               # Higher initial learning rate\n",
    "    momentum=0.9,                          # Momentum for SGD\n",
    "    n_iter_no_change=10,                   # Number of iterations with no improvement to wait before stopping\n",
    "    max_iter=500,                          # Allowing more iterations\n",
    "    random_state=42,\n",
    "    early_stopping=True                    # Enable early stopping\n",
    ")\n",
    "\n",
    "print(\"Starting MLP training...\")\n",
    "mlp.fit(X_train, y_train)  # Fit the model on the full TF-IDF vectors\n",
    "\n",
    "print(\"Evaluating the MLP...\")\n",
    "predictions = mlp.predict(X_val)  # Evaluate on the validation set\n",
    "print(f\"Accuracy: {accuracy_score(y_val, predictions)}\")\n",
    "print(classification_report(y_val, predictions))\n",
    "print(\"Completed MLP training.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sentence_transformers\n",
      "  Downloading sentence_transformers-2.6.1-py3-none-any.whl (163 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m163.3/163.3 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: tqdm in /opt/homebrew/lib/python3.10/site-packages (from sentence_transformers) (4.66.2)\n",
      "Requirement already satisfied: numpy in /opt/homebrew/lib/python3.10/site-packages (from sentence_transformers) (1.26.4)\n",
      "Collecting torch>=1.11.0\n",
      "  Downloading torch-2.2.2-cp310-none-macosx_11_0_arm64.whl (59.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.7/59.7 MB\u001b[0m \u001b[31m28.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: scikit-learn in /opt/homebrew/lib/python3.10/site-packages (from sentence_transformers) (1.4.1.post1)\n",
      "Requirement already satisfied: scipy in /opt/homebrew/lib/python3.10/site-packages (from sentence_transformers) (1.12.0)\n",
      "Collecting huggingface-hub>=0.15.1\n",
      "  Downloading huggingface_hub-0.22.2-py3-none-any.whl (388 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m388.9/388.9 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: Pillow in /opt/homebrew/lib/python3.10/site-packages (from sentence_transformers) (10.2.0)\n",
      "Collecting transformers<5.0.0,>=4.32.0\n",
      "  Downloading transformers-4.39.3-py3-none-any.whl (8.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.8/8.8 MB\u001b[0m \u001b[31m41.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting typing-extensions>=3.7.4.3\n",
      "  Downloading typing_extensions-4.11.0-py3-none-any.whl (34 kB)\n",
      "Collecting filelock\n",
      "  Downloading filelock-3.13.4-py3-none-any.whl (11 kB)\n",
      "Collecting requests\n",
      "  Downloading requests-2.31.0-py3-none-any.whl (62 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.6/62.6 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting fsspec>=2023.5.0\n",
      "  Downloading fsspec-2024.3.1-py3-none-any.whl (171 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m172.0/172.0 kB\u001b[0m \u001b[31m17.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting pyyaml>=5.1\n",
      "  Downloading PyYAML-6.0.1-cp310-cp310-macosx_11_0_arm64.whl (169 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m169.3/169.3 kB\u001b[0m \u001b[31m19.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: packaging>=20.9 in /Users/tay/Library/Python/3.10/lib/python/site-packages (from huggingface-hub>=0.15.1->sentence_transformers) (24.0)\n",
      "Collecting sympy\n",
      "  Downloading sympy-1.12-py3-none-any.whl (5.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.7/5.7 MB\u001b[0m \u001b[31m38.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting jinja2\n",
      "  Downloading Jinja2-3.1.3-py3-none-any.whl (133 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m133.2/133.2 kB\u001b[0m \u001b[31m14.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting networkx\n",
      "  Downloading networkx-3.3-py3-none-any.whl (1.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m37.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /opt/homebrew/lib/python3.10/site-packages (from transformers<5.0.0,>=4.32.0->sentence_transformers) (2023.12.25)\n",
      "Collecting safetensors>=0.4.1\n",
      "  Downloading safetensors-0.4.2-cp310-cp310-macosx_11_0_arm64.whl (393 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m393.4/393.4 kB\u001b[0m \u001b[31m27.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting tokenizers<0.19,>=0.14\n",
      "  Downloading tokenizers-0.15.2-cp310-cp310-macosx_11_0_arm64.whl (2.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m39.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: joblib>=1.2.0 in /opt/homebrew/lib/python3.10/site-packages (from scikit-learn->sentence_transformers) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/homebrew/lib/python3.10/site-packages (from scikit-learn->sentence_transformers) (3.4.0)\n",
      "Collecting MarkupSafe>=2.0\n",
      "  Downloading MarkupSafe-2.1.5-cp310-cp310-macosx_10_9_universal2.whl (18 kB)\n",
      "Collecting urllib3<3,>=1.21.1\n",
      "  Downloading urllib3-2.2.1-py3-none-any.whl (121 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.1/121.1 kB\u001b[0m \u001b[31m13.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting idna<4,>=2.5\n",
      "  Downloading idna-3.6-py3-none-any.whl (61 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.6/61.6 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting certifi>=2017.4.17\n",
      "  Downloading certifi-2024.2.2-py3-none-any.whl (163 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m163.8/163.8 kB\u001b[0m \u001b[31m19.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting charset-normalizer<4,>=2\n",
      "  Downloading charset_normalizer-3.3.2-cp310-cp310-macosx_11_0_arm64.whl (120 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m120.4/120.4 kB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting mpmath>=0.19\n",
      "  Downloading mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m536.2/536.2 kB\u001b[0m \u001b[31m30.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: mpmath, urllib3, typing-extensions, sympy, safetensors, pyyaml, networkx, MarkupSafe, idna, fsspec, filelock, charset-normalizer, certifi, requests, jinja2, torch, huggingface-hub, tokenizers, transformers, sentence_transformers\n",
      "Successfully installed MarkupSafe-2.1.5 certifi-2024.2.2 charset-normalizer-3.3.2 filelock-3.13.4 fsspec-2024.3.1 huggingface-hub-0.22.2 idna-3.6 jinja2-3.1.3 mpmath-1.3.0 networkx-3.3 pyyaml-6.0.1 requests-2.31.0 safetensors-0.4.2 sentence_transformers-2.6.1 sympy-1.12 tokenizers-0.15.2 torch-2.2.2 transformers-4.39.3 typing-extensions-4.11.0 urllib3-2.2.1\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.2.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3.10 -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!{sys.executable} -m pip install sentence_transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 5079/5079 [6:41:06<00:00,  4.74s/it]     \n",
      "Batches: 100%|██████████| 391/391 [04:57<00:00,  1.31it/s]\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "\n",
    "# Load the sentence transformer model\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "def preprocess_text_3(texts, batch_size=128):\n",
    "    \"\"\"\n",
    "    This function takes a list of texts and converts them into word embeddings.\n",
    "    \n",
    "    Parameters:\n",
    "    texts (list): A list of strings.\n",
    "    batch_size (int): The size of batches to process texts. Defaults to 128.\n",
    "    \n",
    "    Returns:\n",
    "    np.ndarray: An array of sentence embeddings.\n",
    "    \"\"\"\n",
    "    # Compute embeddings in batches\n",
    "    embeddings = model.encode(texts, batch_size=batch_size, show_progress_bar=True)\n",
    "    return np.array(embeddings)\n",
    "\n",
    "\n",
    "train_embeddings = preprocess_text_3(train_data['text'].tolist())\n",
    "test_embeddings = preprocess_text_3(test_data['text'].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_embeddings, X_val_embeddings, y_train, y_val = train_test_split(\n",
    "    train_embeddings, \n",
    "    train_data['label'], \n",
    "    test_size=0.2, \n",
    "    random_state=42, \n",
    "    stratify=train_data['label']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting MLP training...\n",
      "Evaluating the MLP...\n",
      "Accuracy: 0.5426846153846154\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.69      0.71      0.70     26000\n",
      "           1       0.46      0.50      0.48     26000\n",
      "           2       0.47      0.38      0.42     26000\n",
      "           3       0.44      0.52      0.48     26000\n",
      "           4       0.67      0.60      0.63     26000\n",
      "\n",
      "    accuracy                           0.54    130000\n",
      "   macro avg       0.55      0.54      0.54    130000\n",
      "weighted avg       0.55      0.54      0.54    130000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "mlp = MLPClassifier(\n",
    "    hidden_layer_sizes=(512, 256),  # Reduced complexity as embeddings are already feature-rich\n",
    "    activation='relu',               # Common choice for activation function\n",
    "    solver='adam',                   # Efficient optimizer\n",
    "    alpha=1e-5,                      # Regularization parameter\n",
    "    batch_size=64,                   # Batch size for training\n",
    "    learning_rate_init=0.001,        # Initial learning rate\n",
    "    max_iter=200,                    # Maximum number of training iterations\n",
    "    random_state=42,                 # Random state for reproducibility\n",
    "    early_stopping=True              # To prevent overfitting\n",
    ")\n",
    "\n",
    "print(\"Starting MLP training...\")\n",
    "mlp.fit(X_train_embeddings, y_train)\n",
    "\n",
    "print(\"Evaluating the MLP...\")\n",
    "predictions = mlp.predict(X_val_embeddings)\n",
    "accuracy = accuracy_score(y_val, predictions)\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(classification_report(y_val, predictions))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting MLP training...\n",
      "Iteration 1, loss = 1.09607862\n",
      "Validation score: 0.534346\n",
      "Iteration 2, loss = 1.05720251\n",
      "Validation score: 0.537231\n",
      "Iteration 3, loss = 1.03924978\n",
      "Validation score: 0.536096\n",
      "Iteration 4, loss = 1.02565714\n",
      "Validation score: 0.542038\n",
      "Iteration 5, loss = 1.01427587\n",
      "Validation score: 0.541519\n",
      "Iteration 6, loss = 1.00372097\n",
      "Validation score: 0.542346\n",
      "Iteration 7, loss = 0.99460339\n",
      "Validation score: 0.543346\n",
      "Iteration 8, loss = 0.98474722\n",
      "Validation score: 0.541558\n",
      "Iteration 9, loss = 0.97708578\n",
      "Validation score: 0.542385\n",
      "Iteration 10, loss = 0.96804657\n",
      "Validation score: 0.537577\n",
      "Iteration 11, loss = 0.96082322\n",
      "Validation score: 0.534788\n",
      "Iteration 12, loss = 0.95340865\n",
      "Validation score: 0.538846\n",
      "Iteration 13, loss = 0.94590561\n",
      "Validation score: 0.538019\n",
      "Iteration 14, loss = 0.93961664\n",
      "Validation score: 0.534288\n",
      "Iteration 15, loss = 0.93296934\n",
      "Validation score: 0.534500\n",
      "Iteration 16, loss = 0.92643198\n",
      "Validation score: 0.532288\n",
      "Iteration 17, loss = 0.92062893\n",
      "Validation score: 0.532442\n",
      "Iteration 18, loss = 0.91504265\n",
      "Validation score: 0.530154\n",
      "Iteration 19, loss = 0.90921194\n",
      "Validation score: 0.528058\n",
      "Iteration 20, loss = 0.90442639\n",
      "Validation score: 0.527962\n",
      "Iteration 21, loss = 0.89920901\n",
      "Validation score: 0.529404\n",
      "Iteration 22, loss = 0.89503808\n",
      "Validation score: 0.527135\n",
      "Iteration 23, loss = 0.89088918\n",
      "Validation score: 0.525673\n",
      "Iteration 24, loss = 0.88568953\n",
      "Validation score: 0.525135\n",
      "Iteration 25, loss = 0.88262437\n",
      "Validation score: 0.527346\n",
      "Iteration 26, loss = 0.87827969\n",
      "Validation score: 0.523673\n",
      "Iteration 27, loss = 0.87420874\n",
      "Validation score: 0.523442\n",
      "Iteration 28, loss = 0.87112346\n",
      "Validation score: 0.521942\n",
      "Validation score did not improve more than tol=0.000100 for 20 consecutive epochs. Stopping.\n",
      "Evaluating the MLP...\n",
      "Accuracy: 0.5420461538461538\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.69      0.71      0.70     26000\n",
      "           1       0.46      0.47      0.47     26000\n",
      "           2       0.44      0.48      0.46     26000\n",
      "           3       0.46      0.40      0.43     26000\n",
      "           4       0.64      0.64      0.64     26000\n",
      "\n",
      "    accuracy                           0.54    130000\n",
      "   macro avg       0.54      0.54      0.54    130000\n",
      "weighted avg       0.54      0.54      0.54    130000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "mlp = MLPClassifier(\n",
    "    hidden_layer_sizes=(256, 256, 256),  # Adjusted architecture\n",
    "    activation='relu',                    \n",
    "    solver='adam',                        \n",
    "    alpha=1e-6,                           # Adjusted regularization\n",
    "    batch_size=256,                       # Adjusted batch size\n",
    "    learning_rate='adaptive',\n",
    "    learning_rate_init=0.005,             # Adjusted learning rate\n",
    "    max_iter=500,                         \n",
    "    random_state=42,\n",
    "    early_stopping=True,\n",
    "    n_iter_no_change=20,                  # More patience for early stopping\n",
    "    shuffle=True,                         # Shuffle data during training\n",
    "    verbose=True                          # For real-time updates\n",
    ")\n",
    "\n",
    "print(\"Starting MLP training...\")\n",
    "mlp.fit(X_train_embeddings, y_train)\n",
    "\n",
    "print(\"Evaluating the MLP...\")\n",
    "predictions = mlp.predict(X_val_embeddings)\n",
    "accuracy = accuracy_score(y_val, predictions)\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(classification_report(y_val, predictions))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting MLP training...\n",
      "Iteration 1, loss = 1.09830630\n",
      "Validation score: 0.529038\n",
      "Iteration 2, loss = 1.05864261\n",
      "Validation score: 0.531250\n",
      "Iteration 3, loss = 1.04069166\n",
      "Validation score: 0.535981\n",
      "Iteration 4, loss = 1.02594769\n",
      "Validation score: 0.536058\n",
      "Iteration 5, loss = 1.01319610\n",
      "Validation score: 0.536462\n",
      "Iteration 6, loss = 1.00077886\n",
      "Validation score: 0.539308\n",
      "Iteration 7, loss = 0.98860957\n",
      "Validation score: 0.534923\n",
      "Iteration 8, loss = 0.97701138\n",
      "Validation score: 0.533327\n",
      "Iteration 9, loss = 0.96582819\n",
      "Validation score: 0.530750\n",
      "Iteration 10, loss = 0.95492898\n",
      "Validation score: 0.531077\n",
      "Iteration 11, loss = 0.94528638\n",
      "Validation score: 0.531596\n",
      "Iteration 12, loss = 0.93482196\n",
      "Validation score: 0.531481\n",
      "Iteration 13, loss = 0.92534444\n",
      "Validation score: 0.525981\n",
      "Iteration 14, loss = 0.91651570\n",
      "Validation score: 0.520827\n",
      "Iteration 15, loss = 0.90755530\n",
      "Validation score: 0.522942\n",
      "Iteration 16, loss = 0.89963139\n",
      "Validation score: 0.522712\n",
      "Iteration 17, loss = 0.89201097\n",
      "Validation score: 0.523673\n",
      "Iteration 18, loss = 0.88423929\n",
      "Validation score: 0.520154\n",
      "Iteration 19, loss = 0.87716685\n",
      "Validation score: 0.518808\n",
      "Iteration 20, loss = 0.87021551\n",
      "Validation score: 0.520442\n",
      "Iteration 21, loss = 0.86370917\n",
      "Validation score: 0.516885\n",
      "Iteration 22, loss = 0.85714347\n",
      "Validation score: 0.521442\n",
      "Iteration 23, loss = 0.85065476\n",
      "Validation score: 0.519500\n",
      "Iteration 24, loss = 0.84518285\n",
      "Validation score: 0.516788\n",
      "Iteration 25, loss = 0.84039406\n",
      "Validation score: 0.510808\n",
      "Iteration 26, loss = 0.83415009\n",
      "Validation score: 0.518692\n",
      "Iteration 27, loss = 0.82975957\n",
      "Validation score: 0.517635\n",
      "Validation score did not improve more than tol=0.000100 for 20 consecutive epochs. Stopping.\n",
      "Evaluating the MLP...\n",
      "Accuracy: 0.5417615384615385\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.66      0.75      0.71     26000\n",
      "           1       0.47      0.44      0.46     26000\n",
      "           2       0.45      0.48      0.46     26000\n",
      "           3       0.47      0.35      0.40     26000\n",
      "           4       0.62      0.69      0.65     26000\n",
      "\n",
      "    accuracy                           0.54    130000\n",
      "   macro avg       0.53      0.54      0.53    130000\n",
      "weighted avg       0.53      0.54      0.53    130000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mlp = MLPClassifier(\n",
    "    hidden_layer_sizes=(512, 512, 256),  # Increased complexity\n",
    "    activation='relu',\n",
    "    solver='adam',\n",
    "    alpha=1e-6,                          # Decreased regularization\n",
    "    batch_size=128,                      # Increased batch size for stability\n",
    "    learning_rate='adaptive',\n",
    "    learning_rate_init=0.005,            # Increased initial learning rate for faster convergence\n",
    "    max_iter=500,\n",
    "    random_state=42,\n",
    "    early_stopping=True,\n",
    "    n_iter_no_change=20,                 # Increased patience for early stopping\n",
    "    shuffle=True,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "print(\"Starting MLP training...\")\n",
    "mlp.fit(X_train_embeddings, y_train)  # Ensure correct dataset is used for training\n",
    "\n",
    "print(\"Evaluating the MLP...\")\n",
    "predictions = mlp.predict(X_val_embeddings)  # Ensure correct dataset is used for evaluation\n",
    "accuracy = accuracy_score(y_val, predictions)\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(classification_report(y_val, predictions))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting MLP training...\n",
      "Iteration 1, loss = 1.08973635\n",
      "Validation score: 0.526038\n",
      "Iteration 2, loss = 1.04996238\n",
      "Validation score: 0.531500\n",
      "Iteration 3, loss = 1.03082357\n",
      "Validation score: 0.538788\n",
      "Iteration 4, loss = 1.01592058\n",
      "Validation score: 0.538442\n",
      "Iteration 5, loss = 1.00225040\n",
      "Validation score: 0.540942\n",
      "Iteration 6, loss = 0.98922393\n",
      "Validation score: 0.538404\n",
      "Iteration 7, loss = 0.97744157\n",
      "Validation score: 0.539577\n",
      "Iteration 8, loss = 0.96543807\n",
      "Validation score: 0.536019\n",
      "Iteration 9, loss = 0.95319704\n",
      "Validation score: 0.535423\n",
      "Iteration 10, loss = 0.94125079\n",
      "Validation score: 0.533058\n",
      "Iteration 11, loss = 0.93010410\n",
      "Validation score: 0.533385\n",
      "Iteration 12, loss = 0.92012071\n",
      "Validation score: 0.528712\n",
      "Iteration 13, loss = 0.90906760\n",
      "Validation score: 0.523769\n",
      "Iteration 14, loss = 0.89968201\n",
      "Validation score: 0.525923\n",
      "Iteration 15, loss = 0.89120890\n",
      "Validation score: 0.523077\n",
      "Iteration 16, loss = 0.88195956\n",
      "Validation score: 0.522423\n",
      "Validation score did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Evaluating the MLP...\n",
      "Accuracy: 0.5420538461538461\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.69      0.72      0.70     26000\n",
      "           1       0.46      0.50      0.48     26000\n",
      "           2       0.46      0.41      0.43     26000\n",
      "           3       0.45      0.46      0.46     26000\n",
      "           4       0.66      0.61      0.64     26000\n",
      "\n",
      "    accuracy                           0.54    130000\n",
      "   macro avg       0.54      0.54      0.54    130000\n",
      "weighted avg       0.54      0.54      0.54    130000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# Apply SMOTE to generate synthetic samples for underrepresented classes\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_smote, y_train_smote = smote.fit_resample(X_train_embeddings, y_train)\n",
    "\n",
    "mlp = MLPClassifier(\n",
    "    hidden_layer_sizes=(256, 256),\n",
    "    activation='relu',\n",
    "    solver='adam',\n",
    "    alpha=1e-6,\n",
    "    batch_size=128,\n",
    "    learning_rate='adaptive',\n",
    "    learning_rate_init=0.002,\n",
    "    max_iter=500,\n",
    "    random_state=42,\n",
    "    early_stopping=True,\n",
    "    n_iter_no_change=10,\n",
    "    shuffle=True,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "print(\"Starting MLP training...\")\n",
    "mlp.fit(X_train_smote, y_train_smote)\n",
    "\n",
    "print(\"Evaluating the MLP...\")\n",
    "predictions = mlp.predict(X_val_embeddings)\n",
    "accuracy = accuracy_score(y_val, predictions)\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(classification_report(y_val, predictions))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting MLP training...\n",
      "Iteration 1, loss = 1.08906656\n",
      "Validation score: 0.530635\n",
      "Iteration 2, loss = 1.04467314\n",
      "Validation score: 0.536019\n",
      "Iteration 3, loss = 1.02197329\n",
      "Validation score: 0.540596\n",
      "Iteration 4, loss = 1.00224999\n",
      "Validation score: 0.541058\n",
      "Iteration 5, loss = 0.98253407\n",
      "Validation score: 0.539096\n",
      "Iteration 6, loss = 0.96123428\n",
      "Validation score: 0.538192\n",
      "Iteration 7, loss = 0.93897989\n",
      "Validation score: 0.537615\n",
      "Iteration 8, loss = 0.91529869\n",
      "Validation score: 0.527481\n",
      "Iteration 9, loss = 0.88964359\n",
      "Validation score: 0.527846\n",
      "Iteration 10, loss = 0.86413847\n",
      "Validation score: 0.527827\n",
      "Iteration 11, loss = 0.83782341\n",
      "Validation score: 0.523500\n",
      "Iteration 12, loss = 0.81278625\n",
      "Validation score: 0.519885\n",
      "Iteration 13, loss = 0.78755523\n",
      "Validation score: 0.517500\n",
      "Iteration 14, loss = 0.76350572\n",
      "Validation score: 0.508558\n",
      "Iteration 15, loss = 0.74145362\n",
      "Validation score: 0.507962\n",
      "Iteration 16, loss = 0.71974565\n",
      "Validation score: 0.504154\n",
      "Iteration 17, loss = 0.69898911\n",
      "Validation score: 0.507423\n",
      "Iteration 18, loss = 0.67897528\n",
      "Validation score: 0.503115\n",
      "Iteration 19, loss = 0.66019809\n",
      "Validation score: 0.499865\n",
      "Iteration 20, loss = 0.64278208\n",
      "Validation score: 0.496923\n",
      "Iteration 21, loss = 0.62640421\n",
      "Validation score: 0.494288\n",
      "Iteration 22, loss = 0.61021460\n",
      "Validation score: 0.494885\n",
      "Iteration 23, loss = 0.59468534\n",
      "Validation score: 0.493519\n",
      "Iteration 24, loss = 0.58082474\n",
      "Validation score: 0.492942\n",
      "Iteration 25, loss = 0.56746507\n",
      "Validation score: 0.485769\n",
      "Validation score did not improve more than tol=0.000100 for 20 consecutive epochs. Stopping.\n",
      "Evaluating the MLP...\n",
      "Accuracy: 0.5455230769230769\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      0.69      0.70     26000\n",
      "           1       0.46      0.54      0.50     26000\n",
      "           2       0.46      0.41      0.43     26000\n",
      "           3       0.46      0.41      0.44     26000\n",
      "           4       0.64      0.66      0.65     26000\n",
      "\n",
      "    accuracy                           0.55    130000\n",
      "   macro avg       0.55      0.55      0.54    130000\n",
      "weighted avg       0.55      0.55      0.54    130000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mlp = MLPClassifier(\n",
    "    hidden_layer_sizes=(512, 512, 256),  # Slightly increased complexity\n",
    "    activation='relu',                    \n",
    "    solver='adam',                        \n",
    "    alpha=1e-5,                           # Adjust regularization if needed\n",
    "    batch_size=128,                       # Fine-tune batch size\n",
    "    learning_rate='adaptive',             \n",
    "    learning_rate_init=0.001,             # Adjust initial learning rate if needed\n",
    "    max_iter=500,                         # Increase if early stopping is too early\n",
    "    random_state=42,                      \n",
    "    early_stopping=True,                  \n",
    "    n_iter_no_change=20,                  # Increase patience\n",
    "    shuffle=True,                         \n",
    "    verbose=True                          \n",
    ")\n",
    "\n",
    "print(\"Starting MLP training...\")\n",
    "mlp.fit(X_train_embeddings, y_train_smote)  # Use SMOTE adjusted training data\n",
    "\n",
    "print(\"Evaluating the MLP...\")\n",
    "predictions = mlp.predict(X_val_embeddings)  # Validation on original unaltered validation set\n",
    "accuracy = accuracy_score(y_val, predictions)\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(classification_report(y_val, predictions))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting MLP training...\n",
      "Iteration 1, loss = 1.09154737\n",
      "Validation score: 0.528865\n",
      "Iteration 2, loss = 1.04365931\n",
      "Validation score: 0.535096\n",
      "Iteration 3, loss = 1.02139267\n",
      "Validation score: 0.539269\n",
      "Iteration 4, loss = 1.00238764\n",
      "Validation score: 0.540731\n",
      "Iteration 5, loss = 0.98432301\n",
      "Validation score: 0.540346\n",
      "Iteration 6, loss = 0.96569958\n",
      "Validation score: 0.538692\n",
      "Iteration 7, loss = 0.94567652\n",
      "Validation score: 0.536154\n",
      "Iteration 8, loss = 0.92438808\n",
      "Validation score: 0.532635\n",
      "Iteration 9, loss = 0.90193195\n",
      "Validation score: 0.526962\n",
      "Iteration 10, loss = 0.87770450\n",
      "Validation score: 0.525923\n",
      "Iteration 11, loss = 0.85211266\n",
      "Validation score: 0.523250\n",
      "Iteration 12, loss = 0.82569055\n",
      "Validation score: 0.520212\n",
      "Iteration 13, loss = 0.79956040\n",
      "Validation score: 0.514442\n",
      "Iteration 14, loss = 0.77422549\n",
      "Validation score: 0.509615\n",
      "Iteration 15, loss = 0.74874428\n",
      "Validation score: 0.508942\n",
      "Iteration 16, loss = 0.72401716\n",
      "Validation score: 0.508481\n",
      "Iteration 17, loss = 0.70114682\n",
      "Validation score: 0.503865\n",
      "Iteration 18, loss = 0.67861484\n",
      "Validation score: 0.503231\n",
      "Iteration 19, loss = 0.65756298\n",
      "Validation score: 0.499615\n",
      "Iteration 20, loss = 0.63783750\n",
      "Validation score: 0.499173\n",
      "Iteration 21, loss = 0.61846340\n",
      "Validation score: 0.493904\n",
      "Iteration 22, loss = 0.60015448\n",
      "Validation score: 0.495673\n",
      "Iteration 23, loss = 0.58306650\n",
      "Validation score: 0.493615\n",
      "Iteration 24, loss = 0.56534686\n",
      "Validation score: 0.491096\n",
      "Iteration 25, loss = 0.55150806\n",
      "Validation score: 0.487596\n",
      "Iteration 26, loss = 0.53521879\n",
      "Validation score: 0.487788\n",
      "Iteration 27, loss = 0.52194487\n",
      "Validation score: 0.486788\n",
      "Iteration 28, loss = 0.50808099\n",
      "Validation score: 0.483096\n",
      "Iteration 29, loss = 0.49482643\n",
      "Validation score: 0.484481\n",
      "Iteration 30, loss = 0.48162489\n",
      "Validation score: 0.481019\n",
      "Validation score did not improve more than tol=0.000100 for 25 consecutive epochs. Stopping.\n",
      "Evaluating the MLP...\n",
      "Accuracy: 0.5451384615384616\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.70      0.71      0.71     26000\n",
      "           1       0.45      0.54      0.49     26000\n",
      "           2       0.45      0.43      0.44     26000\n",
      "           3       0.47      0.40      0.43     26000\n",
      "           4       0.65      0.65      0.65     26000\n",
      "\n",
      "    accuracy                           0.55    130000\n",
      "   macro avg       0.54      0.55      0.54    130000\n",
      "weighted avg       0.54      0.55      0.54    130000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mlp = MLPClassifier(\n",
    "    hidden_layer_sizes=(512, 512, 256),  # Continue with a slightly complex model\n",
    "    activation='relu',                    \n",
    "    solver='adam',                        \n",
    "    alpha=1e-4,                           # Increase regularization slightly if needed\n",
    "    batch_size=128,                       # Fine-tune batch size\n",
    "    learning_rate='adaptive',             \n",
    "    learning_rate_init=0.0005,            # Lower initial learning rate\n",
    "    max_iter=500,                         # Allow for more iterations\n",
    "    random_state=42,                      \n",
    "    early_stopping=True,                  \n",
    "    n_iter_no_change=25,                  # Increase patience\n",
    "    shuffle=True,                         \n",
    "    verbose=True                          \n",
    ")\n",
    "\n",
    "print(\"Starting MLP training...\")\n",
    "mlp.fit(X_train_embeddings, y_train_smote)  # Ensure you use the SMOTE-adjusted training data\n",
    "\n",
    "print(\"Evaluating the MLP...\")\n",
    "predictions = mlp.predict(X_val_embeddings)  # Validate on the original unaltered validation set\n",
    "accuracy = accuracy_score(y_val, predictions)\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(classification_report(y_val, predictions))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 5079/5079 [1:11:01<00:00,  1.19it/s] \n",
      "Batches: 100%|██████████| 391/391 [27:03<00:00,  4.15s/it]   \n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split, cross_val_predict\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Load the sentence transformer model for embeddings\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# Define a function for preprocessing text into embeddings\n",
    "def preprocess_text_3(texts, batch_size=128):\n",
    "    embeddings = model.encode(texts, batch_size=batch_size, show_progress_bar=True)\n",
    "    return np.array(embeddings)\n",
    "\n",
    "# Process the text data into embeddings\n",
    "train_embeddings = preprocess_text_3(train_data['text'].tolist())\n",
    "test_embeddings = preprocess_text_3(test_data['text'].tolist())\n",
    "\n",
    "# Also, get TF-IDF features\n",
    "tfidf = TfidfVectorizer(max_features=5000)\n",
    "X_train_tfidf = tfidf.fit_transform(train_data['text'])\n",
    "X_test_tfidf = tfidf.transform(test_data['text'])\n",
    "\n",
    "# Combine TF-IDF features with embeddings\n",
    "X_train_combined = np.hstack((X_train_tfidf.toarray(), train_embeddings))\n",
    "X_test_combined = np.hstack((X_test_tfidf.toarray(), test_embeddings))\n",
    "\n",
    "# Split into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_train_combined, \n",
    "    train_data['label'], \n",
    "    test_size=0.2, \n",
    "    random_state=42, \n",
    "    stratify=train_data['label']\n",
    ")\n",
    "\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "\n",
    "# Initialize the MLP\n",
    "mlp = MLPClassifier(\n",
    "    hidden_layer_sizes=(128, 64),\n",
    "    activation='relu',\n",
    "    max_iter=1,  # We'll manually iterate over epochs\n",
    "    warm_start=True,  # Keep training the same model\n",
    "    solver='adam',\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Define batch size and calculate number of batches\n",
    "batch_size = 32\n",
    "n_batches = int(np.ceil(X_train.shape[0] / batch_size))\n",
    "\n",
    "# Training with manual mini-batches\n",
    "for epoch in range(100):  # Replace with the number of epochs you want\n",
    "    X_train_shuffled, y_train_shuffled = shuffle(X_train, y_train, random_state=epoch)\n",
    "    \n",
    "    for batch in range(n_batches):\n",
    "        start = batch * batch_size\n",
    "        end = start + batch_size\n",
    "        X_batch, y_batch = X_train_shuffled[start:end], y_train_shuffled[start:end]\n",
    "        mlp.partial_fit(X_batch, y_batch, classes=np.unique(y_train))\n",
    "\n",
    "    # Early stopping logic here: evaluate on X_val, break if no improvement\n",
    "\n",
    "# Evaluate the final model\n",
    "predictions = mlp.predict(X_val)\n",
    "accuracy = accuracy_score(y_val, predictions)\n",
    "print(f\"Final accuracy: {accuracy}\")\n",
    "print(classification_report(y_val, predictions))\n",
    "\n",
    "\n",
    "# Plot the confusion matrix\n",
    "cm = confusion_matrix(y_val, predictions)\n",
    "plt.figure(figsize=(10, 7))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.show()\n",
    "\n",
    "# Perform cross-validated predictions for ensembling\n",
    "print(\"Starting cross-validated predictions for ensembling...\")\n",
    "cv_predictions = cross_val_predict(mlp, X_train, y_train, cv=5, method='predict_proba')\n",
    "\n",
    "# Ensemble predictions by averaging probabilities\n",
    "average_predictions = np.argmax(cv_predictions, axis=1)\n",
    "\n",
    "# Evaluate ensemble performance\n",
    "ensemble_accuracy = accuracy_score(y_train, average_predictions)\n",
    "print(f\"Ensemble accuracy: {ensemble_accuracy}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
